input:
  label: kafka_input_2
  kafka_franz:
    seed_brokers:
      - ${KAFKA_BOOTSTRAP_SERVERS:localhost}:${KAFKA_BOOTSTRAP_PORT:9092}
    topics:
      - '${DEVICE_VALID_CE_TOPIC:topic.device.validcloudevents}'
    consumer_group: "zone.dimo.dps.validcloudevents2"
    client_id: ${CONTAINER_NAME:localhost}-dps-valid-ce-2
    rack_id: ${NODE_NAME:localhost}
    fetch_max_bytes: 100MiB
    fetch_max_wait: 50ms
    fetch_min_bytes: 1B
    fetch_max_partition_bytes: 10MiB
    commit_period: 5s
    checkpoint_limit: 1000000 # double the batch size for insertion
    start_offset: "latest"


pipeline:
  processors:
    - label: "file_index_migration_2"
      dimo_file_index_db_migration:
        dsn: clickhouse://${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}/${CLICKHOUSE_INDEX_DATABASE}?username=${CLICKHOUSE_USER}&password=${CLICKHOUSE_PASSWORD}&secure=true&dial_timeout=5s

    - label: "inputlogger_2"
      for_each:
        - log:
            level: DEBUG
            message: 'MessageReceived'
            fields_mapping: |
              root.payload = this

output:
  label: "insert_valid_cloudevent_2"
  fallback:
    - broker:
        pattern: fan_out_sequential_fail_fast
        outputs:
          # 1. Write payload to Parquet on object storage.
          #    Sets dimo_cloudevent_index metadata to parquet_path#row_offset for the ClickHouse index.
          #    Sets dimo_parquet_path/size/count on the first message for the Iceberg commit step.
          - fallback:
              - label: "write_valid_cloudevent_parquet_2"
                dimo_parquet_writer:
                  warehouse: ${ICEBERG_WAREHOUSE:s3://dimo-iceberg-dev/warehouse/}
                  prefix: cloudevent/valid/
                  storage_endpoint: ${OBJECT_STORAGE_ENDPOINT:https://s3.us-east-2.amazonaws.com}
                  credentials:
                    access_key: ${OBJECT_STORAGE_ACCESS_KEY:${S3_AWS_ACCESS_KEY_ID}}
                    secret_key: ${OBJECT_STORAGE_SECRET_KEY:${S3_AWS_SECRET_ACCESS_KEY}}
                  batching:
                    count: 10000
                    period: "5s"
              - label: 'write_valid_cloudevent_parquet_failure_2'
                reject: '${!metadata("fallback_error").or("failed to write cloudevent parquet")}'
                processors:
                  -  mutation: |
                       meta dimo_component = "write_valid_cloudevent_parquet_2"
                  - resource: "handle_db_error"

          # 2. Publish Iceberg commit message for the Parquet file.
          #    Only the first message per batch carries dimo_parquet_path; the rest are dropped.
          - fallback:
              - label: "publish_iceberg_commit_valid_2"
                kafka_franz:
                  seed_brokers:
                    - '${KAFKA_BOOTSTRAP_SERVERS:localhost}:${KAFKA_BOOTSTRAP_PORT:9092}'
                  topic: '${ICEBERG_COMMIT_TOPIC:topic.iceberg.commits}'
                processors:
                  - mapping: |
                      let ppath = metadata("dimo_parquet_path").or("")
                      if $ppath == "" { root = deleted() }
                      root.file_path = $ppath
                      root.file_size = metadata("dimo_parquet_size").or("0").number()
                      root.record_count = metadata("dimo_parquet_count").or("0").number()
                      root.namespace = "cloudevent"
                      root.table_name = "valid"
              - label: 'publish_iceberg_commit_failure_valid_2'
                reject: '${!metadata("fallback_error").or("failed to publish iceberg commit")}'
                processors:
                  -  mutation: |
                       meta dimo_component = "publish_iceberg_commit_valid_2"
                  - resource: "handle_db_error"

          # 3. Insert index into ClickHouse cloud_event_2 table.
          #    index_key now points to parquet_path#row_offset instead of individual S3 file path.
          - fallback:
              - label: "insert_valid_cloudevent_clickhouse_2"
                sql_insert:
                  driver: clickhouse
                  dsn: clickhouse://${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}/${CLICKHOUSE_INDEX_DATABASE}?username=${CLICKHOUSE_USER}&password=${CLICKHOUSE_PASSWORD}&secure=true&dial_timeout=5s&max_execution_time=300
                  table: cloud_event_2
                  columns: []
                  args_mapping: root = this
                  batching:
                    count: 500000
                    byte_size: 0
                    period: "2s"
                    check: ""
                processors:
                  - label: "split_values_valid_2"
                    dimo_split_values: {}
                  - catch:
                      -  mutation: |
                           meta dimo_component = "split_values_valid_2"
                      - resource: "handle_db_error"
                      - mapping: root = deleted()

              - label: 'insert_valid_cloudevent_clickhouse_failure_2'
                switch:
                  cases:
                  # If the error contains "bad connection", reject the message so it will be retried
                    - check: 'metadata("fallback_error").or("").contains("bad connection")'
                      output:
                        reject: metadata("fallback_error")
                        processors:
                          - resource: "handle_db_connection_error"
                    - check: ''
                      output:
                        drop: {}
                        processors:
                          -  mutation: |
                              meta dimo_component = "insert_valid_cloudevent_clickhouse_2"
                          - resource: "handle_db_error"
    - drop: {}
