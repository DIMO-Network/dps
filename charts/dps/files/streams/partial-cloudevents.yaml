input:
  label: kafka_input
  kafka_franz:
    seed_brokers:
      - ${KAFKA_BOOTSTRAP_SERVERS:localhost}:${KAFKA_BOOTSTRAP_PORT:9092}
    topics:
      - '${DEVICE_PARTIAL_CE_TOPIC:topic.device.partialcloudevents}'
    consumer_group: "zone.dimo.dps.partialcloudevents"
    client_id: ${CONTAINER_NAME:localhost}-dps-partial-ce
    rack_id: ${NODE_NAME:localhost}
    fetch_max_bytes: 100MiB
    fetch_max_wait: 50ms
    fetch_min_bytes: 1B
    fetch_max_partition_bytes: 10MiB
    commit_period: 2s
    checkpoint_limit: 1000000 # double the batch size for insertion
    start_offset: "latest"


pipeline:
  processors:
    - label: "inputlogger"
      for_each:
        - log:
            level: DEBUG
            message: 'MessageReceived'
            fields_mapping: |
              root.payload = this

output:
  label: "insert_partial_cloudevent"
  fallback:
    - broker:
        pattern: fan_out_sequential_fail_fast
        outputs:
          - fallback:
              - label: "insert_partial_cloudevent_s3"
                aws_s3:
                  bucket: ${S3_EPHEMERAL_BUCKET}
                  path: ${!meta("dimo_cloudevent_index")}
                  content_type: application/json
                  region: ${S3_AWS_REGION}
                  max_in_flight: 250
                  timeout: 30s
                  metadata:
                    exclude_prefixes: ["dimo_cloudevent_index"]
                  credentials:
                    id: ${S3_AWS_ACCESS_KEY_ID}
                    secret: ${S3_AWS_SECRET_ACCESS_KEY}
                  batching:
                    count: 10000
                    period: "5s"
              # reject the message, log, record metrics
              # reject instead of drop to avoid attempting CH index insert with no backing obj
              - label: 'insert_partial_cloudevent_s3_failure'
                reject: '${!metadata("fallback_error").or("failed to store converted cloudevent")}'
                # processor is happening first, so we can use it to set the meta
                processors:
                  -  mutation: |
                       meta dimo_component = "insert_partial_cloudevent_s3"
                  - resource: "handle_db_error"

          - fallback:
              - label: "insert_partial_cloudevent_clickhouse"
                sql_insert:
                  driver: clickhouse
                  dsn: clickhouse://${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}/${CLICKHOUSE_INDEX_DATABASE}?username=${CLICKHOUSE_USER}&password=${CLICKHOUSE_PASSWORD}&secure=true&dial_timeout=5s&max_execution_time=300
                  table: cloud_event
                  columns: []
                  args_mapping: root = this
                  batching:
                    count: 500000
                    byte_size: 0
                    period: "2s"
                    check: ""
                processors:
                  - label: "split_values_partial"
                    dimo_split_values: {}
                  - catch:
                      -  mutation: |
                           meta dimo_component = "split_values_partial"
                      - resource: "handle_db_error"
                      - mapping: root = deleted()

              - label: 'insert_partial_cloudevent_clickhouse_failure'
                switch:
                  cases:
                  # If the error contains "bad connection", reject the message so it will be retried
                    - check: 'metadata("fallback_error").or("").contains("bad connection")'
                      output:
                        reject: metadata("fallback_error")
                        processors:
                          - resource: "handle_db_connection_error"
                    - check: ''
                      output:
                        drop: {}
                        processors:
                          -  mutation: |
                              meta dimo_component = "insert_signal_clickhouse"
                          - resource: "handle_db_error"
    - drop: {}
