input:
  label: kafka_input
  kafka_franz:
    seed_brokers:
      - ${KAFKA_BOOTSTRAP_SERVERS:localhost}:${KAFKA_BOOTSTRAP_PORT:9092}
    topics:
      - '${DEVICE_SIGNALS_TOPIC:topic.device.signals}'
    consumer_group: "zone.dimo.dps.signals"
    client_id: ${CONTAINER_NAME:localhost}-dps-signals
    rack_id: ${NODE_NAME:localhost}
    fetch_max_bytes: 100MiB
    fetch_max_wait: 50ms
    fetch_min_bytes: 1B
    fetch_max_partition_bytes: 10MiB
    commit_period: 2s
    checkpoint_limit: 1000000 # double the batch size for insertion
    start_offset: "latest"

pipeline:
  processors:
    - label: signal_db_migration
      dimo_signal_migration:
        dsn: clickhouse://${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}/${CLICKHOUSE_SIGNAL_DATABASE}?username=${CLICKHOUSE_USER}&password=${CLICKHOUSE_PASSWORD}&secure=true&dial_timeout=5s

    - label: "inputlogger"
      for_each:
        - log:
            level: DEBUG
            message: 'MessageReceived'
            fields_mapping: |
              root.payload = this
    - label: "convert_to_slice"
      signal_to_slice:

output:
  label: "insert_signal"
  fallback:
    - label: "insert_signal_clickhouse"
      sql_insert:
        driver: clickhouse
        dsn: clickhouse://${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}/${CLICKHOUSE_SIGNAL_DATABASE}?username=${CLICKHOUSE_USER}&password=${CLICKHOUSE_PASSWORD}&secure=true&max_execution_time=600
        table: signal
        columns: []
        args_mapping: root = this
        batching:
          count: 500000
          byte_size: 0
          period: "2s"
          check: ""
    # Drop the message, log, record metrics
    - label: "insert_signal_clickhouse_failure"
      drop: {}
      processors:
        -  mutation: |
             meta dimo_component = "insert_signal_clickhouse"
        - resource: "handle_db_error"
